{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "http://igbo.von.gov.ng/ subdomains text webscraper for Akuko and Egwuregwu SUBDOMAINS."
      ],
      "metadata": {
        "id": "peRbgyHeGSyy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naijiria pages"
      ],
      "metadata": {
        "id": "3f6RhE3VwdgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "data2=[]\n",
        "# Define the URL of the starting page\n",
        "start_url =  \"https://igbo.von.gov.ng/2023/09/\"\n",
        "# Keep track of the URLs we've already visited\n",
        "visited_urls = set()\n",
        "\n",
        "# Define a function to scrape URLs from a page\n",
        "def scrape_urls(url):\n",
        "    # Send an HTTP request to the server\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML response\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        body_text = soup.get_text()\n",
        "\n",
        "################\n",
        "        # Find all the paragraphs in the HTML content\n",
        "        paragraphs = soup.find_all(\"p\")\n",
        "\n",
        "        # Extract the text from the paragraphs\n",
        "        text = [p.text for p in paragraphs]\n",
        "\n",
        "        # Join the text into a single string\n",
        "        text = \" \".join(text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################\n",
        "        # Add the body text and URL to the dataframe\n",
        "        data2.append({\"url\": url, \"body_text\": body_text, 'paragraph': text})\n",
        "\n",
        "        # Find all links on the page\n",
        "        links = soup.find_all(\"a\")\n",
        "        soup.decode('utf-8', 'ignore')\n",
        "\n",
        "        # Loop through each link\n",
        "        for link in links:\n",
        "            # Extract the link's URL\n",
        "            link_url = link.get(\"href\")\n",
        "\n",
        "            # Check if the link URL is not None\n",
        "            if link_url:\n",
        "                # Check if the link is internal to the domain\n",
        "                if link_url.startswith(start_url):\n",
        "                    # Check if the link has not been visited yet\n",
        "                    if link_url not in visited_urls:\n",
        "                        # Add the link to the list of visited URLs\n",
        "                        visited_urls.add(link_url)\n",
        "\n",
        "                        # Recursively scrape the linked page\n",
        "                        scrape_urls(link_url)\n",
        "\n",
        "# Start the scraping process\n",
        "scrape_urls(start_url)\n",
        "df2 = pd.DataFrame(data2)\n",
        "\n",
        "# Print the list of visited URLs\n",
        "print(len(visited_urls))"
      ],
      "metadata": {
        "id": "MhRIVcQdcLLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "df2['paragraph'] = df2['paragraph'].str.replace(r'Prev Post.*', '', regex=True, flags=re.DOTALL)\n",
        "df2['paragraph'] = df2['paragraph'].str.replace(r'Publisher.*\\n', '', regex=True)\n",
        "\n",
        "df2['paragraph'] = df2['paragraph'].str.split(' \\n\\n\\n\\n').str[0].str.strip()\n",
        "\n",
        "\n",
        "print(df2['paragraph'])\n"
      ],
      "metadata": {
        "id": "ZlIAHZIeBs34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title saving Akuko corpus as Akuko.csv under the folder on the left panel\n",
        "df2[['url','paragraph']].to_csv('2023-09.csv',encoding='utf-8-sig')"
      ],
      "metadata": {
        "id": "leiAieedBqiG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}